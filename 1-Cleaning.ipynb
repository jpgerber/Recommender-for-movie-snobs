{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving movie recommendations for movie snobs - I. Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project examines how to improve the precision of recommender systems for movie snobs. This section covers an exploratory analysis of the ratings dataset.\n",
    "\n",
    "Section I covers the following tasks:\n",
    "1. Extracting the basic ratings data and canonical (1,001 Movies...) list. \n",
    "2. Wrangling (including inserting the canonical list into the movie ratings dataset).\n",
    "3. Joining the Grouplens tables together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initial summary of ratings and canonical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five datasets of relevance on the Grouplens website.\n",
    "\n",
    "| Name | Movies | Ratings | Userdata | Release date |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| New research | 27K | 20M | None | 2016 |\n",
    "| Education (small) | 9K | 100K | None | 2018 |\n",
    "| Education (large) | 58K | 27M | None | 2018 |\n",
    "| Older (100K) | 1.7K | 100K| age, gender, occupation, zip | 1998 |\n",
    "| Older (1M) | 4K  | 1M | age, gender, occupation, zip | 2003 |\n",
    "\n",
    "The latter two are useful for identifying clusters of people but are a bit small, while the first and third are best if trying to identify features of movies themselves. Given the obscurity of some of the canonical movies, we chose the largest dataset, Education (large).\n",
    "\n",
    "The <a href='https://1001films.fandom.com/wiki/The_List'> canonical list</a> from \"1,001 Movies...\" had 1,222 entries due to additions and deletions across the different editions..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packagesused\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the Grouplens data (in this case just one file, but we did all of them as a list)\n",
    "\n",
    "#import requests, zipfile, io\n",
    "# Importing the ratings data\n",
    "# First download and extract the files (there's a bunch so use a list and loop)\n",
    "#list_of_urls = ['http://files.grouplens.org/datasets/movielens/ml-latest.zip']\n",
    "#for url in list_of_urls:\n",
    "#    ratings_small_file = requests.get(url)\n",
    "#    z = zipfile.ZipFile(io.BytesIO(ratings_small_file.content))\n",
    "#    z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Importing the 1,001 list and converting it to a list\n",
    "snob_url = 'https://1001films.fandom.com/wiki/The_List'\n",
    "snob_text= requests.get(snob_url)\n",
    "soup = BeautifulSoup(snob_text.content, 'html.parser')\n",
    "basic_list = (soup.body.find_all('b'))\n",
    "thousand_list = [item.text for item in basic_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I get all three files into dataframes\n",
    "# first, read the correct downloaded csvs (from separate subfolders)\n",
    "ed_large_movies = pd.read_csv('ml-latest/movies.csv', sep = ',', header = 0)\n",
    "\n",
    "# The two older files have a different compression structure and require a different technique\n",
    "#older_small_movies = pd.read_csv('ml-100k/movies.csv', sep = ',', header = 0)\n",
    "#older_large_movies = pd.read_csv('ml-1m/movies.csv', sep = ',', header = 0)\n",
    "\n",
    "# Converting the 1,001 list to a dataframe and dropping the header row\n",
    "thousandone_movies = pd.DataFrame(thousand_list, columns = ['title']).drop(0)\n",
    "\n",
    "# The following code can be modified to check they are all dataframes\n",
    "#print(thousandone_movies.head())\n",
    "#print(ed_large_movies.head())\n",
    "#thousandone_movies.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wrangling (including inserting the canonical list into the ratings data)\n",
    "\n",
    "#### 2a. Wrangling the canonical list.\n",
    "The canonical list came as a set of strings, so we needed to extract the year of release from those strings and then find a way to match the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns of movie years in each database\n",
    "# Make sure the titles don't have trailing spaces\n",
    "thousandone_movies['title'] = thousandone_movies['title'].str.rstrip()\n",
    "ed_large_movies['title'] = ed_large_movies['title'].str.rstrip()\n",
    "# Then take the slices (the years are in parantheses at the end of the title)\n",
    "thousandone_movies['year'] = [title[slice(-5,-1)] for title in thousandone_movies['title']]\n",
    "ed_large_movies['year'] = [title[slice(-5,-1)] for title in ed_large_movies['title']]\n",
    "\n",
    "# Then convert these strings to numbers (there is one title missing a year!)\n",
    "# Define a conversion function\n",
    "def ConvertYear(value):\n",
    "    '''This function converts integer strings to integers and non-integer strings to zero'''\n",
    "    try:\n",
    "        return int(value)\n",
    "    except: \n",
    "        return 0\n",
    "# Then apply it to the columns for both thousandone_movies and ed_choices\n",
    "thousandone_movies['year'] = thousandone_movies['year'].apply(lambda year: ConvertYear(\n",
    "    year))\n",
    "ed_large_movies['year'] = ed_large_movies['year'].apply(lambda year: ConvertYear(year))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Inserting the canonical list into the ratings data.\n",
    "No doubt the hardest wrangling job was adjusting for any naming discrepancies in titles between files. I tried straight matching first joined the movie sets to the 1,001 list and the match rate was 49%. Then I tried some simple string manipulations and brought it up to 72%. I then used the nltk package's tokenizer, and an 80% match rate (974 out of 1,222)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fuzzywuzzy, a string matching library, compares a string to a set of targets and returns the best match. The basic extractOne method led to the highest match rate yet (91.6%), but 103 mismatches remained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell does the first version of the fuzzywuzzy matching using the default sorter\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Select the list of titles\n",
    "choices_ed = ed_large_movies['title']\n",
    "\n",
    "# Test it on a small part\n",
    "#test_titles = thousandone_movies.iloc[0:30]\n",
    "\n",
    "def Matcher(title, choices):\n",
    "    title_match, percent_match, match3 = process.extractOne(title, choices)\n",
    "    return title_match\n",
    "\n",
    "# original version (103 mismatches)\n",
    "thousandone_movies['return_match'] = thousandone_movies['title'].apply(lambda title: Matcher(\n",
    "    title, choices_ed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching using fuzzywuzzy's in-built tokenizer led to only 48 mismatches.\n",
    "I then inspected the mismatches and noticed that most were to movies with similar titles but very different years (e.g. October/Oktyabr (1929) matched to October Sky (1999)). As such, I thought to limit the number of targets to ±1 year from the known year of release (exact year would not work as sometimes the databases differed in their year of release). Having a reduced matching list also greatly sped up the matching algorithm. To do this, I filtered the matching list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The matching done via fuzzywuzzy's tokenizer sorting algorithm.\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Get the list of possible matches\n",
    "\n",
    "choices_ed = ed_large_movies['title']\n",
    "\n",
    "# Define the matching function correcly\n",
    "def Matcher(title, choices):\n",
    "    title_match, percent_match, match3 = process.extractOne(title, choices)\n",
    "    return title_match\n",
    "\n",
    "def Matcher_token(title, choices):\n",
    "    title_match, percent_match, match3 = process.extractOne(title, choices, \n",
    "                                                            scorer=fuzz.token_sort_ratio)\n",
    "    return title_match\n",
    "\n",
    "# Call it on all the rows we wish to have matched via a lambda function\n",
    "thousandone_movies['return_match'] = thousandone_movies['title'].apply(lambda title: Matcher_token\n",
    "                                                                       (title, choices_ed))\n",
    "# Save the file for checking\n",
    "thousandone_movies.to_csv('MatcherWithFuzzyWuzzyToken.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Here's the final version of the matching </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.3 s, sys: 538 ms, total: 46.8 s\n",
      "Wall time: 54.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "# Specify the matching function (we only need one of the outputs)\n",
    "def Matcher(title, choices):\n",
    "    title_match, percent_match, match3 = process.extractOne(title, choices)\n",
    "    return title_match\n",
    "# And here's a function for using the tokenizer\n",
    "def Matcher_token(title, choices):\n",
    "    title_match, percent_match, match3 = process.extractOne(title, choices, \n",
    "                                                            scorer=fuzz.token_sort_ratio)\n",
    "    return title_match\n",
    "\n",
    "#Define a filter to return targets for +/-1 year only\n",
    "def YearFilter (year):\n",
    "    years = [year-1, year, year+1]\n",
    "    return ed_large_movies[ed_large_movies.year.isin(years)].title\n",
    "\n",
    "# Running the tokenizer over the filtered target set\n",
    "for index, row in thousandone_movies.iterrows():\n",
    "    # call the filter\n",
    "    targets = YearFilter(row.year)\n",
    "    # update the new cell work out the matcher\n",
    "    thousandone_movies.loc[index,'return_match'] = Matcher_token(row.title, targets)\n",
    "thousandone_movies.to_csv('thousandone_forlooped_matcher.csv')\n",
    "\n",
    "# I then had to visually inspect the forlooped_matcher file to calculate the number of mismatches (38/1224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can ignore the cell below, it was for testing purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            title  year                    return_match\n",
      "2  The Great Train Robbery (1903)  1903  The Great Train Robbery (1903)\n",
      "3    The Birth of a Nation (1915)  1915   Birth of a Nation, The (1915)\n",
      "4             Les Vampires (1915)  1915            Vampires, Les (1915)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathangerber/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/Users/jonathangerber/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "''' This cell contains a test version of the matching routine'''\n",
    "#Testing one title\n",
    "test_title = thousandone_movies.iloc[1:2]\n",
    "#test_title = thousandone_movies.iloc[1:2].drop(['return_match'], axis=1)\n",
    "#print(test_title)\n",
    "#Testing two titles\n",
    "test_titles = thousandone_movies.iloc[1:4]\n",
    "#test_titles = thousandone_movies.iloc[1:3].drop(['return_match'], axis=1)\n",
    "#print(test_title)\n",
    "\n",
    "#def ChoiceList(array_of_targets):\n",
    "#    return YearFilter(array_of_targets.year).title\n",
    "\n",
    "Train_Targets = YearFilter(test_title.year)\n",
    "#print(Train_Targets)\n",
    "#x = Matcher('The Melo', Train_Targets)\n",
    "\n",
    "# Running the basic function over the whole database\n",
    "for index, row in test_titles.iterrows():\n",
    "    # call the filter\n",
    "    targets = YearFilter(row.year)\n",
    "#    print(targets)\n",
    "    # update the new cell work out the matcher\n",
    "    test_titles.loc[index,'return_match'] = Matcher_token(row.title, targets)\n",
    "    \n",
    "# This one works\n",
    "#test_titles['return_match'] = test_titles['title'].apply(lambda x: \n",
    "#                                                       Matcher_token(x, YearFilter(test_title.year)))\n",
    "\n",
    "print(test_titles)\n",
    "\n",
    "#Great Train Robbery <- gets the right comp (1902-1904)\n",
    "#Birth of a nation <- uses the comparison set from the first row (1902-1904)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The films were now matched in the 1,001 movie file but this file needed to be joined to the Grouplens data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  year  canonical  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  1995        1.0  \n",
      "1                   Adventure|Children|Fantasy  1995        0.0  \n",
      "2                               Comedy|Romance  1995        0.0  \n",
      "3                         Comedy|Drama|Romance  1995        0.0  \n",
      "4                                       Comedy  1995        0.0  \n"
     ]
    }
   ],
   "source": [
    "# Add the indicator variable to the canonical list.\n",
    "thousandone_movies['canonical'] = 1\n",
    "#print(thousandone_movies.head())\n",
    "\n",
    "# Add the canonical indicator to the movie file, drop the irrelevant columns \n",
    "#and fill the missing values with zeroes\n",
    "ed_large_movies = pd.merge(ed_large_movies, thousandone_movies, left_on='title', right_on='return_match', how='outer', \n",
    "         suffixes=('', '_canon')).drop(['year_canon', \n",
    "                                        'return_match', 'title_canon'], axis=1).fillna({'canonical':0})\n",
    "\n",
    "print(ed_large_movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c. Updating the release date\n",
    "I used an API wrapper to replace the year of release with date of release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathangerber/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/jonathangerber/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 is processed\n",
      "Batch 1 is processed\n",
      "Batch 2 is processed\n",
      "Batch 3 is processed\n",
      "Batch 4 is processed\n",
      "Batch 5 is processed\n",
      "Batch 6 is processed\n",
      "Batch 7 is processed\n",
      "Batch 8 is processed\n",
      "Batch 9 is processed\n",
      "Batch 10 is processed\n",
      "Batch 11 is processed\n",
      "Batch 12 is processed\n",
      "Batch 13 is processed\n",
      "Batch 14 is processed\n",
      "Batch 15 is processed\n",
      "Batch 16 is processed\n",
      "Batch 17 is processed\n",
      "Batch 18 is processed\n",
      "Batch 19 is processed\n",
      "Batch 20 is processed\n",
      "Batch 21 is processed\n",
      "Batch 22 is processed\n",
      "Batch 23 is processed\n",
      "Batch 24 is processed\n",
      "Batch 25 is processed\n",
      "Batch 26 is processed\n",
      "Batch 27 is processed\n",
      "Batch 28 is processed\n",
      "Batch 29 is processed\n",
      "Batch 30 is processed\n",
      "Batch 31 is processed\n",
      "Batch 32 is processed\n",
      "Batch 33 is processed\n",
      "Batch 34 is processed\n",
      "Batch 35 is processed\n",
      "Batch 36 is processed\n",
      "Batch 37 is processed\n",
      "Batch 38 is processed\n",
      "Batch 39 is processed\n",
      "Batch 40 is processed\n",
      "Batch 41 is processed\n",
      "Batch 42 is processed\n",
      "Batch 43 is processed\n",
      "Batch 44 is processed\n",
      "Batch 45 is processed\n",
      "Batch 46 is processed\n",
      "Batch 47 is processed\n",
      "Batch 48 is processed\n",
      "Batch 49 is processed\n",
      "Batch 50 is processed\n",
      "Batch 51 is processed\n",
      "Batch 52 is processed\n",
      "Batch 53 is processed\n",
      "Batch 54 is processed\n",
      "Batch 55 is processed\n",
      "Batch 56 is processed\n",
      "Batch 57 is processed\n",
      "Batch 58 is processed\n",
      "Batch 59 is processed\n",
      "Batch 60 is processed\n",
      "Batch 61 is processed\n",
      "Batch 62 is processed\n",
      "Batch 63 is processed\n",
      "Batch 64 is processed\n",
      "Batch 65 is processed\n",
      "Batch 66 is processed\n",
      "Batch 67 is processed\n",
      "Batch 68 is processed\n",
      "Batch 69 is processed\n",
      "Batch 70 is processed\n",
      "Batch 71 is processed\n",
      "Batch 72 is processed\n",
      "Batch 73 is processed\n",
      "Batch 74 is processed\n",
      "Batch 75 is processed\n",
      "Batch 76 is processed\n",
      "Batch 77 is processed\n",
      "Batch 78 is processed\n",
      "Batch 79 is processed\n",
      "Batch 80 is processed\n",
      "Batch 81 is processed\n",
      "Batch 82 is processed\n",
      "Batch 83 is processed\n",
      "Batch 84 is processed\n",
      "Batch 85 is processed\n",
      "Batch 86 is processed\n",
      "Batch 87 is processed\n",
      "Batch 88 is processed\n",
      "Batch 89 is processed\n",
      "Batch 90 is processed\n",
      "Batch 91 is processed\n",
      "Batch 92 is processed\n",
      "Batch 93 is processed\n",
      "Batch 94 is processed\n",
      "Batch 95 is processed\n",
      "Batch 96 is processed\n",
      "Batch 97 is processed\n",
      "Batch 98 is processed\n",
      "Batch 99 is processed\n",
      "Batch 100 is processed\n",
      "Batch 101 is processed\n",
      "Batch 102 is processed\n",
      "Batch 103 is processed\n",
      "Batch 104 is processed\n",
      "Batch 105 is processed\n",
      "Batch 106 is processed\n",
      "Batch 107 is processed\n",
      "Batch 108 is processed\n",
      "Batch 109 is processed\n",
      "Batch 110 is processed\n",
      "Batch 111 is processed\n",
      "Batch 112 is processed\n",
      "Batch 113 is processed\n",
      "Batch 114 is processed\n",
      "Batch 115 is processed\n",
      "Batch 116 is processed\n",
      "Batch 117 is processed\n",
      "Batch 118 is processed\n",
      "Batch 119 is processed\n",
      "Batch 120 is processed\n",
      "Batch 121 is processed\n",
      "Batch 122 is processed\n",
      "Batch 123 is processed\n",
      "Batch 124 is processed\n",
      "Batch 125 is processed\n",
      "Batch 126 is processed\n",
      "Batch 127 is processed\n",
      "Batch 128 is processed\n",
      "Batch 129 is processed\n",
      "Batch 130 is processed\n",
      "Batch 131 is processed\n",
      "Batch 132 is processed\n",
      "Batch 133 is processed\n",
      "Batch 134 is processed\n",
      "Batch 135 is processed\n",
      "Batch 136 is processed\n",
      "Batch 137 is processed\n",
      "Batch 138 is processed\n",
      "Batch 139 is processed\n",
      "Batch 140 is processed\n",
      "Batch 141 is processed\n",
      "Batch 142 is processed\n",
      "Batch 143 is processed\n",
      "Batch 144 is processed\n",
      "Batch 145 is processed\n",
      "Batch 146 is processed\n",
      "Batch 147 is processed\n",
      "Batch 148 is processed\n",
      "Batch 149 is processed\n",
      "Batch 150 is processed\n",
      "Batch 151 is processed\n",
      "Batch 152 is processed\n",
      "Batch 153 is processed\n",
      "Batch 154 is processed\n",
      "Batch 155 is processed\n",
      "Batch 156 is processed\n",
      "Batch 157 is processed\n",
      "Batch 158 is processed\n",
      "Batch 159 is processed\n",
      "Batch 160 is processed\n",
      "Batch 161 is processed\n",
      "Batch 162 is processed\n",
      "Batch 163 is processed\n",
      "Batch 164 is processed\n",
      "Batch 165 is processed\n",
      "Batch 166 is processed\n",
      "Batch 167 is processed\n",
      "Batch 168 is processed\n",
      "Batch 169 is processed\n",
      "Batch 170 is processed\n",
      "Batch 171 is processed\n",
      "Batch 172 is processed\n",
      "Batch 173 is processed\n",
      "Batch 174 is processed\n",
      "Batch 175 is processed\n",
      "Batch 176 is processed\n",
      "Batch 177 is processed\n",
      "Batch 178 is processed\n",
      "Batch 179 is processed\n",
      "Batch 180 is processed\n",
      "Batch 181 is processed\n",
      "Batch 182 is processed\n",
      "Batch 183 is processed\n",
      "Batch 184 is processed\n",
      "Batch 185 is processed\n",
      "Batch 186 is processed\n",
      "Batch 187 is processed\n",
      "Batch 188 is processed\n",
      "Batch 189 is processed\n",
      "Batch 190 is processed\n",
      "Batch 191 is processed\n",
      "Batch 192 is processed\n",
      "Batch 193 is processed\n",
      "Batch 194 is processed\n",
      "Batch 195 is processed\n",
      "Batch 196 is processed\n",
      "Batch 197 is processed\n",
      "Batch 198 is processed\n",
      "Batch 199 is processed\n",
      "Batch 200 is processed\n",
      "Batch 201 is processed\n",
      "Batch 202 is processed\n",
      "Batch 203 is processed\n",
      "Batch 204 is processed\n",
      "Batch 205 is processed\n",
      "Batch 206 is processed\n",
      "Batch 207 is processed\n",
      "Batch 208 is processed\n",
      "Batch 209 is processed\n",
      "Batch 210 is processed\n",
      "Batch 211 is processed\n",
      "Batch 212 is processed\n",
      "Batch 213 is processed\n",
      "Batch 214 is processed\n",
      "Batch 215 is processed\n",
      "Batch 216 is processed\n",
      "Batch 217 is processed\n",
      "Batch 218 is processed\n",
      "Batch 219 is processed\n",
      "Batch 220 is processed\n",
      "Batch 221 is processed\n",
      "Batch 222 is processed\n",
      "Batch 223 is processed\n",
      "Batch 224 is processed\n",
      "Batch 225 is processed\n",
      "Batch 226 is processed\n",
      "Batch 227 is processed\n",
      "Batch 228 is processed\n",
      "Batch 229 is processed\n",
      "Batch 230 is processed\n",
      "Batch 231 is processed\n",
      "Batch 232 is processed\n",
      "Batch 233 is processed\n",
      "Batch 234 is processed\n",
      "Batch 235 is processed\n",
      "Batch 236 is processed\n",
      "Batch 237 is processed\n",
      "Batch 238 is processed\n",
      "Batch 239 is processed\n",
      "Batch 240 is processed\n",
      "Batch 241 is processed\n",
      "Batch 242 is processed\n",
      "Batch 243 is processed\n",
      "Batch 244 is processed\n",
      "Batch 245 is processed\n",
      "Batch 246 is processed\n",
      "Batch 247 is processed\n",
      "Batch 248 is processed\n",
      "Batch 249 is processed\n",
      "Batch 250 is processed\n",
      "Batch 251 is processed\n",
      "Batch 252 is processed\n",
      "Batch 253 is processed\n",
      "Batch 254 is processed\n",
      "Batch 255 is processed\n",
      "Batch 256 is processed\n",
      "Batch 257 is processed\n",
      "Batch 258 is processed\n",
      "Batch 259 is processed\n",
      "Batch 260 is processed\n",
      "Batch 261 is processed\n",
      "Batch 262 is processed\n",
      "Batch 263 is processed\n",
      "Batch 264 is processed\n",
      "Batch 265 is processed\n",
      "Batch 266 is processed\n",
      "Batch 267 is processed\n",
      "Batch 268 is processed\n",
      "Batch 269 is processed\n",
      "Batch 270 is processed\n",
      "Batch 271 is processed\n",
      "Batch 272 is processed\n",
      "Batch 273 is processed\n",
      "Batch 274 is processed\n",
      "Batch 275 is processed\n",
      "Batch 276 is processed\n",
      "Batch 277 is processed\n",
      "Batch 278 is processed\n",
      "Batch 279 is processed\n",
      "Batch 280 is processed\n",
      "Batch 281 is processed\n",
      "Batch 282 is processed\n",
      "Batch 283 is processed\n",
      "Batch 284 is processed\n",
      "Batch 285 is processed\n",
      "Batch 286 is processed\n",
      "Batch 287 is processed\n",
      "Batch 288 is processed\n",
      "Batch 289 is processed\n",
      "Batch 290 is processed\n",
      "Batch 291 is processed\n",
      "Batch 292 is processed\n",
      "Batch 293 is processed\n",
      "Batch 294 is processed\n",
      "Batch 295 is processed\n",
      "Batch 296 is processed\n",
      "Batch 297 is processed\n",
      "Batch 298 is processed\n",
      "Batch 299 is processed\n",
      "Batch 300 is processed\n",
      "Batch 301 is processed\n",
      "Batch 302 is processed\n",
      "Batch 303 is processed\n",
      "Batch 304 is processed\n",
      "Batch 305 is processed\n",
      "Batch 306 is processed\n",
      "Batch 307 is processed\n",
      "Batch 308 is processed\n",
      "Batch 309 is processed\n",
      "Batch 310 is processed\n",
      "Batch 311 is processed\n",
      "Batch 312 is processed\n",
      "Batch 313 is processed\n",
      "Batch 314 is processed\n",
      "Batch 315 is processed\n",
      "Batch 316 is processed\n",
      "Batch 317 is processed\n",
      "Batch 318 is processed\n",
      "Batch 319 is processed\n",
      "Batch 320 is processed\n",
      "Batch 321 is processed\n",
      "Batch 322 is processed\n",
      "Batch 323 is processed\n",
      "Batch 324 is processed\n",
      "Batch 325 is processed\n",
      "Batch 326 is processed\n",
      "Batch 327 is processed\n",
      "Batch 328 is processed\n",
      "Batch 329 is processed\n",
      "Batch 330 is processed\n",
      "Batch 331 is processed\n",
      "Batch 332 is processed\n",
      "Batch 333 is processed\n",
      "Batch 334 is processed\n",
      "Batch 335 is processed\n",
      "Batch 336 is processed\n",
      "Batch 337 is processed\n",
      "Batch 338 is processed\n",
      "Batch 339 is processed\n",
      "Batch 340 is processed\n",
      "Batch 341 is processed\n",
      "Batch 342 is processed\n",
      "Batch 343 is processed\n",
      "Batch 344 is processed\n",
      "Batch 345 is processed\n",
      "Batch 346 is processed\n",
      "Batch 347 is processed\n",
      "Batch 348 is processed\n",
      "Batch 349 is processed\n",
      "Batch 350 is processed\n",
      "Batch 351 is processed\n",
      "Batch 352 is processed\n",
      "Batch 353 is processed\n",
      "Batch 354 is processed\n",
      "Batch 355 is processed\n",
      "Batch 356 is processed\n",
      "Batch 357 is processed\n",
      "Batch 358 is processed\n",
      "Batch 359 is processed\n",
      "Batch 360 is processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 361 is processed\n",
      "Batch 362 is processed\n",
      "Batch 363 is processed\n",
      "Batch 364 is processed\n",
      "Batch 365 is processed\n",
      "Batch 366 is processed\n",
      "Batch 367 is processed\n",
      "Batch 368 is processed\n",
      "Batch 369 is processed\n",
      "Batch 370 is processed\n",
      "Batch 371 is processed\n",
      "Batch 372 is processed\n",
      "Batch 373 is processed\n",
      "Batch 374 is processed\n",
      "Batch 375 is processed\n",
      "Batch 376 is processed\n",
      "Batch 377 is processed\n",
      "Batch 378 is processed\n",
      "Batch 379 is processed\n",
      "Batch 380 is processed\n",
      "Batch 381 is processed\n",
      "Batch 382 is processed\n",
      "Batch 383 is processed\n",
      "Batch 384 is processed\n",
      "Batch 385 is processed\n",
      "Batch 386 is processed\n",
      "Batch 387 is processed\n",
      "Batch 388 is processed\n",
      "Batch 389 is processed\n",
      "Batch 390 is processed\n",
      "Batch 391 is processed\n",
      "Batch 392 is processed\n",
      "Batch 393 is processed\n",
      "Batch 394 is processed\n",
      "Batch 395 is processed\n",
      "Batch 396 is processed\n",
      "Batch 397 is processed\n",
      "Batch 398 is processed\n",
      "Batch 399 is processed\n",
      "Batch 400 is processed\n",
      "Batch 401 is processed\n",
      "Batch 402 is processed\n",
      "Batch 403 is processed\n",
      "Batch 404 is processed\n",
      "Batch 405 is processed\n",
      "Batch 406 is processed\n",
      "Batch 407 is processed\n",
      "Batch 408 is processed\n",
      "Batch 409 is processed\n",
      "Batch 410 is processed\n",
      "Batch 411 is processed\n",
      "Batch 412 is processed\n",
      "Batch 413 is processed\n",
      "Batch 414 is processed\n",
      "Batch 415 is processed\n",
      "Batch 416 is processed\n",
      "Batch 417 is processed\n",
      "Batch 418 is processed\n",
      "Batch 419 is processed\n",
      "Batch 420 is processed\n",
      "Batch 421 is processed\n",
      "Batch 422 is processed\n",
      "Batch 423 is processed\n",
      "Batch 424 is processed\n",
      "Batch 425 is processed\n",
      "Batch 426 is processed\n",
      "Batch 427 is processed\n",
      "Batch 428 is processed\n",
      "Batch 429 is processed\n",
      "Batch 430 is processed\n",
      "Batch 431 is processed\n",
      "Batch 432 is processed\n",
      "Batch 433 is processed\n",
      "Batch 434 is processed\n",
      "Batch 435 is processed\n",
      "Batch 436 is processed\n",
      "Batch 437 is processed\n",
      "Batch 438 is processed\n",
      "Batch 439 is processed\n",
      "Batch 440 is processed\n",
      "Batch 441 is processed\n",
      "Batch 442 is processed\n",
      "Batch 443 is processed\n",
      "Batch 444 is processed\n",
      "Batch 445 is processed\n",
      "Batch 446 is processed\n",
      "Batch 447 is processed\n",
      "Batch 448 is processed\n",
      "Batch 449 is processed\n",
      "Batch 450 is processed\n",
      "Batch 451 is processed\n",
      "Batch 452 is processed\n",
      "Batch 453 is processed\n",
      "Batch 454 is processed\n",
      "Batch 455 is processed\n",
      "Batch 456 is processed\n",
      "Batch 457 is processed\n",
      "Batch 458 is processed\n",
      "Batch 459 is processed\n",
      "Batch 460 is processed\n",
      "Batch 461 is processed\n",
      "Batch 462 is processed\n",
      "Batch 463 is processed\n",
      "Batch 464 is processed\n",
      "Batch 465 is processed\n",
      "Batch 466 is processed\n",
      "Batch 467 is processed\n",
      "Batch 468 is processed\n",
      "Batch 469 is processed\n",
      "Batch 470 is processed\n",
      "Batch 471 is processed\n",
      "Batch 472 is processed\n",
      "Batch 473 is processed\n",
      "Batch 474 is processed\n",
      "Batch 475 is processed\n",
      "Batch 476 is processed\n",
      "Batch 477 is processed\n",
      "Batch 478 is processed\n",
      "Batch 479 is processed\n",
      "Batch 480 is processed\n",
      "Batch 481 is processed\n",
      "Batch 482 is processed\n",
      "Batch 483 is processed\n",
      "Batch 484 is processed\n",
      "Batch 485 is processed\n",
      "Batch 486 is processed\n",
      "Batch 487 is processed\n",
      "Batch 488 is processed\n",
      "Batch 489 is processed\n",
      "Batch 490 is processed\n",
      "Batch 491 is processed\n",
      "Batch 492 is processed\n",
      "Batch 493 is processed\n",
      "Batch 494 is processed\n",
      "Batch 495 is processed\n",
      "Batch 496 is processed\n",
      "Batch 497 is processed\n",
      "Batch 498 is processed\n",
      "Batch 499 is processed\n",
      "Batch 500 is processed\n",
      "Batch 501 is processed\n",
      "Batch 502 is processed\n",
      "Batch 503 is processed\n",
      "Batch 504 is processed\n",
      "Batch 505 is processed\n",
      "Batch 506 is processed\n",
      "Batch 507 is processed\n",
      "Batch 508 is processed\n",
      "Batch 509 is processed\n",
      "Batch 510 is processed\n",
      "Batch 511 is processed\n",
      "Batch 512 is processed\n",
      "Batch 513 is processed\n",
      "Batch 514 is processed\n",
      "Batch 515 is processed\n",
      "Batch 516 is processed\n",
      "Batch 517 is processed\n",
      "Batch 518 is processed\n",
      "Batch 519 is processed\n",
      "Batch 520 is processed\n",
      "Batch 521 is processed\n",
      "Batch 522 is processed\n",
      "Batch 523 is processed\n",
      "Batch 524 is processed\n",
      "Batch 525 is processed\n",
      "Batch 526 is processed\n",
      "Batch 527 is processed\n",
      "Batch 528 is processed\n",
      "Batch 529 is processed\n",
      "Batch 530 is processed\n",
      "Batch 531 is processed\n",
      "Batch 532 is processed\n",
      "Batch 533 is processed\n",
      "Batch 534 is processed\n",
      "Batch 535 is processed\n",
      "Batch 536 is processed\n",
      "Batch 537 is processed\n",
      "Batch 538 is processed\n",
      "Batch 539 is processed\n",
      "Batch 540 is processed\n",
      "Batch 541 is processed\n",
      "Batch 542 is processed\n",
      "Batch 543 is processed\n",
      "Batch 544 is processed\n",
      "Batch 545 is processed\n",
      "Batch 546 is processed\n",
      "Batch 547 is processed\n",
      "Batch 548 is processed\n",
      "Batch 549 is processed\n",
      "Batch 550 is processed\n",
      "Batch 551 is processed\n",
      "Batch 552 is processed\n",
      "Batch 553 is processed\n",
      "Batch 554 is processed\n",
      "Batch 555 is processed\n",
      "Batch 556 is processed\n",
      "Batch 557 is processed\n",
      "Batch 558 is processed\n",
      "Batch 559 is processed\n",
      "Batch 560 is processed\n",
      "Batch 561 is processed\n",
      "Batch 562 is processed\n",
      "Batch 563 is processed\n",
      "Batch 564 is processed\n",
      "Batch 565 is processed\n",
      "Batch 566 is processed\n",
      "Batch 567 is processed\n",
      "Batch 568 is processed\n",
      "Batch 569 is processed\n",
      "Batch 570 is processed\n",
      "Batch 571 is processed\n",
      "Batch 572 is processed\n",
      "Batch 573 is processed\n",
      "Batch 574 is processed\n",
      "Batch 575 is processed\n",
      "Batch 576 is processed\n",
      "Batch 577 is processed\n",
      "Batch 578 is processed\n",
      "Batch 579 is processed\n",
      "Batch 580 is processed\n"
     ]
    }
   ],
   "source": [
    "# Insert the correct tmdb movie IDs into the Grouplens file\n",
    "tmdb_links = pd.read_csv('ml-latest/links.csv', sep=',', header=0)\n",
    "\n",
    "ed_large_movies = pd.merge(ed_large_movies, tmdb_links, left_on='movieId', right_on='movieId', how='outer', \n",
    "         suffixes=('', 'tmdb')).drop(['imdbId'], axis=1)\n",
    "\n",
    "# Use the tmdb IDs to get the correct release dates via the tmdbsimple API library\n",
    "import tmdbsimple as tmdb\n",
    "tmdb.API_KEY = 'b70779efd983d6156620d110a8d3b3c4'\n",
    "\n",
    "# Also need to limit the rate for the call\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=7, period=2)\n",
    "def ReleaseDate(tmdb_ID):\n",
    "    try: \n",
    "        movie = tmdb.Movies(tmdb_ID)\n",
    "        response = movie.info()\n",
    "        return movie.release_date\n",
    "    except:\n",
    "        return np.nan\n",
    "%time\n",
    "\n",
    "# Here I make a looping function to check that it works\n",
    "\n",
    "ed_large_movies['release_date'] = np.NaN\n",
    "i = 0\n",
    "batchsize = 100\n",
    "calls = ed_large_movies.shape[0] // batchsize\n",
    "\n",
    "for batch in range(0,calls):\n",
    "    j = i + 100\n",
    "    ed_large_movies.ix[i:j,'release_date'] = ed_large_movies.ix[i:j,'tmdbId'].apply(\n",
    "    lambda x: ReleaseDate(x))\n",
    "    i+=batchsize\n",
    "    print('Batch {:.0f} is processed'.format(batch))\n",
    "\n",
    "#for batch in range(0,batchlist):\n",
    "    \n",
    "#for \n",
    "#ed_large_movies.ix[0:10,'release_date'] = ed_large_movies.ix[0:10,'tmdbId'].apply(\n",
    "#    lambda x: ReleaseDate(x))\n",
    "\n",
    "#print(ed_large_movies.head())\n",
    "\n",
    "#ed_large_movies['release_date'] = ed_large_movies['tmdbId'].apply(lambda x:ReleaseDate(x))\n",
    "#canonical_ed = ed_large_movies[ed_large_movies['canonical']==1]\n",
    "#canonical_ed['release_date'] = canonical_ed['tmdbId'].apply(lambda x: ReleaseDate(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ed_large_movies.tail())\n",
    "\n",
    "# write the resulting file for later use\n",
    "ed_large_movies.to_csv('RightDates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  year  canonical   tmdbId  \\\n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  1995        1.0    862.0   \n",
      "1                   Adventure|Children|Fantasy  1995        0.0   8844.0   \n",
      "2                               Comedy|Romance  1995        0.0  15602.0   \n",
      "3                         Comedy|Drama|Romance  1995        0.0  31357.0   \n",
      "4                                       Comedy  1995        0.0  11862.0   \n",
      "\n",
      "  release_date  \n",
      "0   1995-10-30  \n",
      "1   1995-12-15  \n",
      "2   1995-12-22  \n",
      "3   1995-12-22  \n",
      "4   1995-12-08  \n"
     ]
    }
   ],
   "source": [
    "# read the file again\n",
    "ed_large_movies = pd.read_csv('RightDates.csv')\n",
    "\n",
    "# Remove some duplicate columns\n",
    "ed_large_movies = ed_large_movies.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'tmdbIdtmdb'])\n",
    "\n",
    "# Check the file\n",
    "print(ed_large_movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Joining the grouplens tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Loading other files\n",
    "The Grouplens sets have several linked tables. \n",
    "The 'tags' file contains a list of tags derived from reviews and user tags. It also gives the date when each tag was given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId           tag   timestamp\n",
      "0      14      110          epic  1443148538\n",
      "1      14      110      Medieval  1443148532\n",
      "2      14      260        sci-fi  1442169410\n",
      "3      14      260  space action  1442169421\n",
      "4      14      318  imdb top 250  1442615195\n",
      "(1108997, 4)\n",
      "   userId  movieId           tag   timestamp                date\n",
      "0      14      110          epic  1443148538 2015-09-24 22:35:38\n",
      "1      14      110      Medieval  1443148532 2015-09-24 22:35:32\n",
      "2      14      260        sci-fi  1442169410 2015-09-13 14:36:50\n",
      "3      14      260  space action  1442169421 2015-09-13 14:37:01\n",
      "4      14      318  imdb top 250  1442615195 2015-09-18 18:26:35\n"
     ]
    }
   ],
   "source": [
    "tags = pd.read_csv('ml-latest/tags.csv', sep = ',', header = 0)\n",
    "print(tags.head())\n",
    "print(tags.shape)\n",
    "\n",
    "from datetime import datetime\n",
    "tags['date'] = tags['timestamp'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "np.min(tags['date'])\n",
    "\n",
    "print(tags.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'ratings' file contains a list of ratings by user and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The earliest movie rating was made on 1995-01-09 06:46:44\n",
      "The latest movie rating was made on 2018-09-26 02:59:09\n",
      "(27753444, 5)\n",
      "   userId  movieId  rating   timestamp                date\n",
      "0       1      307     3.5  1256677221 2009-10-27 17:00:21\n",
      "1       1      481     3.5  1256677456 2009-10-27 17:04:16\n",
      "2       1     1091     1.5  1256677471 2009-10-27 17:04:31\n",
      "3       1     1257     4.5  1256677460 2009-10-27 17:04:20\n",
      "4       1     1449     4.5  1256677264 2009-10-27 17:01:04\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv('ml-latest/ratings.csv', sep = ',', header = 0)\n",
    "ratings['date'] = ratings['timestamp'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "print('The earliest movie rating was made on {}'.format(np.min(ratings['date'])))\n",
    "print('The latest movie rating was made on {}'.format(np.max(ratings['date'])))\n",
    "print(ratings.shape)\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'genome-score' file contains loadings of the tag set to each movie (see Vig et al reference). This genome appears to be built from a relatively small sample from an academic paper. The 'genome-tag' file contains identifying information for each tag. The 'links' file contains identifying information for each movie title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genome score header\n",
      "   movieId  tagId  relevance\n",
      "0        1      1    0.02900\n",
      "1        1      2    0.02375\n",
      "2        1      3    0.05425\n",
      "3        1      4    0.06875\n",
      "4        1      5    0.16000\n",
      "Genome tags header\n",
      "   tagId           tag\n",
      "0      1           007\n",
      "1      2  007 (series)\n",
      "2      3  18th century\n",
      "3      4         1920s\n",
      "4      5         1930s\n",
      "Link header\n",
      "   movieId  imdbId   tmdbId\n",
      "0        1  114709    862.0\n",
      "1        2  113497   8844.0\n",
      "2        3  113228  15602.0\n",
      "3        4  114885  31357.0\n",
      "4        5  113041  11862.0\n"
     ]
    }
   ],
   "source": [
    "# Briefly looking at last three files\n",
    "genome_scores = pd.read_csv('ml-latest/genome-scores.csv', sep = ',', header = 0)\n",
    "print('Genome score header')\n",
    "print(genome_scores.head())\n",
    "genome_tags = pd.read_csv('ml-latest/genome-tags.csv', sep = ',', header = 0)\n",
    "print('Genome tags header')\n",
    "print(genome_tags.head())\n",
    "tmdb_links = pd.read_csv('ml-latest/links.csv', sep=',', header=0)\n",
    "print('Link header')\n",
    "print(tmdb_links.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Joining all the files.\n",
    "The current analysis needs only one file, a long-format file of ratings with all the necessary identifying movie data (including canonical status and date of release)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Relevant join to get the file we need.\n",
    "\n",
    "# First doublecheck the file structure\n",
    "#print(ratings.head())\n",
    "#print(ed_large_movies.head())\n",
    "\n",
    "# Here's the join (have to drop duplicates in the ed_large list or else some ratings are counted twice)\n",
    "rating2 = pd.merge(ratings, ed_large_movies.drop_duplicates(subset=['movieId']), how='left', on='movieId').drop(\n",
    "    ['title','genres'], axis=1)  # we also dropped the long string columns\n",
    "\n",
    "# A simple check to make sure we didn't get any extra lines\n",
    "print(ratings.shape[0]==rating2.shape[0])\n",
    "\n",
    "# Then save that dataframe for later use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the full file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store the file for use in later sections of this report\n",
    "rating2.to_pickle('Pickled_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data file we will use is now cleaned and wrangled.\n",
    "\n",
    "References:\n",
    "Jesse Vig, Shilad Sen, and John Riedl. 2012. The Tag Genome: Encoding Community Knowledge to Support Novel Interaction. ACM Trans. Interact. Intell. Syst. 2, 3: 13:1–13:44. https://doi.org/10.1145/2362394.2362395"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
