{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving movie recommendations for movie snobs (draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project examines how to improve the precision of recommender systems for movie snobs.\n",
    "\n",
    "I will go through the following tasks:\n",
    "1. Collate the 1,001 Movies list\n",
    "2. Select a dataset that has most of the 1,001 Movies in it\n",
    "3. Do some exploratory analyses of differences between 1,001 and non-1,001 movies (and The Dekalog and The Dark Knight), potentially by using demographic information\n",
    "4. Do some baseline recommender modeling\n",
    "5. Run a baseline model and then a model with 1,001 as a topic/rating.\n",
    "6. Compare the models (with a 10% holdout set)\n",
    "7. Try various models until we can predict the difference between 1,001 and non-1,001 movies \n",
    "\n",
    "Other ideas:\n",
    "1. Work out how long it takes for something to be canonical and then use this as a bias adjustment.\n",
    "2. use rotten tomatoes api to find difference b/w dek and dk reviews (base it on the mini-project for naive bayes section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five datasets of relevance on the Grouplens website.\n",
    "\n",
    "| Name | Movies | Ratings | Userdata | Release date |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| New research | 27K | 20M | None | 2016 |\n",
    "| Education (small) | 9K | 100K | None | 2018 |\n",
    "| Education (large) | 58K | 27M | None | 2018 |\n",
    "| Older (100K) | 1.7K | 100K| age, gender, occupation, zip | 1998 |\n",
    "| Older (1M) | 4K  | 1M | age, gender, occupation, zip | 2003 |\n",
    "\n",
    "The latter two are useful for identifying people, the second and fourth are best if trying to look at changes in movie status across time. But the latter two also have a very different file structure requiring a separate decompression method.\n",
    "\n",
    "The relative percentages of the 1,001 Movies in each will be the first thing to work out. To do this, the 1,001 list needs to be broadcast to each dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packagesused\n",
    "import pandas as pd\n",
    "import requests, zipfile, io\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the ratings data\n",
    "# First download and extract the files (there's a bunch so use a list and loop)\n",
    "list_of_urls = ['http://files.grouplens.org/datasets/movielens/ml-20m.zip',\n",
    "               'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip',\n",
    "               'http://files.grouplens.org/datasets/movielens/ml-latest.zip',\n",
    "               'http://files.grouplens.org/datasets/movielens/ml-100k.zip',\n",
    "               'http://files.grouplens.org/datasets/movielens/ml-1m.zip']\n",
    "for url in list_of_urls:\n",
    "    ratings_small_file = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(ratings_small_file.content))\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7361f0de35d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mthousand_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbasic_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthousand_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# Importing the 1,001 list and converting it to a list\n",
    "snob_url = 'https://1001films.fandom.com/wiki/The_List'\n",
    "snob_text= requests.get(snob_url)\n",
    "soup = BeautifulSoup(snob_text.content, 'html.parser')\n",
    "basic_list = (soup.body.find_all('b'))\n",
    "thousand_list = [item.text for item in basic_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I get all three files into dataframes\n",
    "# first, read the correct downloaded csvs (from separate subfolders)\n",
    "new_movies = pd.read_csv('ml-20m/movies.csv', sep = ',', header = 0)\n",
    "ed_small_movies = pd.read_csv('ml-latest-small/movies.csv', sep = ',', header = 0)\n",
    "ed_large_movies = pd.read_csv('ml-latest/movies.csv', sep = ',', header = 0)\n",
    "\n",
    "# The two older files have a different compression structure and require a different technique\n",
    "#older_small_movies = pd.read_csv('ml-100k/movies.csv', sep = ',', header = 0)\n",
    "#older_large_movies = pd.read_csv('ml-1m/movies.csv', sep = ',', header = 0)\n",
    "\n",
    "# Converting the 1,001 list to a dataframe and dropping the header row\n",
    "thousandone_movies = pd.DataFrame(thousand_list, columns = ['title']).drop(0)\n",
    "\n",
    "# The following code can be modified to check they are all dataframes\n",
    "#print(type(small_set_movies))\n",
    "# Looking at head of all six\n",
    "#print(thousandone_movies.head())\n",
    "#print(new_movies.head())\n",
    "#print(ed_small_movies.head())\n",
    "#print(ed_large_movies.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No doubt the hardest wrangling job was adjusting for any naming discrepancies between files. I first joined the movie sets to the 1,001 list to check how many matched (via a dropna command). The match rate was 60%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 545 entries, 7 to 1175\n",
      "Data columns (total 3 columns):\n",
      "title      545 non-null object\n",
      "movieId    545 non-null float64\n",
      "genres     545 non-null object\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 17.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 485 entries, 1 to 1221\n",
      "Data columns (total 3 columns):\n",
      "title      485 non-null object\n",
      "movieId    485 non-null float64\n",
      "genres     485 non-null object\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 15.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 602 entries, 1 to 1222\n",
      "Data columns (total 3 columns):\n",
      "title      602 non-null object\n",
      "movieId    602 non-null float64\n",
      "genres     602 non-null object\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 18.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Do a left join of the small Grouplens set to the 1,001 set\n",
    "new_movies_join = thousandone_movies.merge(new_movies, on='title', how='left')\n",
    "ed_small_join = thousandone_movies.merge(ed_small_movies, on='title', how='left')\n",
    "ed_large_join = thousandone_movies.merge(ed_large_movies, on='title', how='left')\n",
    "\n",
    "#print(small_join.head(50))\n",
    "#print(large_join.head(50))\n",
    "\n",
    "# Demonstrating how many matched in each file\n",
    "new_movies_join.dropna().info()\n",
    "ed_small_join.dropna().info()\n",
    "ed_large_join.dropna().info()\n",
    "\n",
    "# Save the large to Excel to inspect why they don't match\n",
    "#new_movies_join.to_csv('NewMoviesChecker.csv')\n",
    "ed_large_join.to_csv('EdLargeChecker.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I worked through wrangling the largest of the movie sets with the 1,001 list.\n",
    "\n",
    "After comparing the first 50 missing values in the large education file, I worked out that most errors were due to slight errors in formatting such as prepositions, punctuation, capitalization discrepancies, or year being off by one. Each of those could be corrected by string methods, which led to 883 matches (88%). Better but not good enough. (In reading up on how to do this via sentence similarity matching, I realized that a tokenizer is quicker.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting for capitalization (make everything lowercase)\n",
    "\n",
    "# Prepositions must be done before capitalization because otherwise trailing a's will be removed\n",
    "# Correcting for preposition order in 1,001 (1001 has capitalized prepositions at the start)\n",
    "#thousandone_movies['title_edit'] = thousandone_movies['title'].str.replace('A ', '', 1)\n",
    "#thousandone_movies['title_edit'] = thousandone_movies['title_edit'].str.replace('The ', '', 1)\n",
    "#thousandone_movies['title_edit'] = thousandone_movies['title_edit'].str.replace('Les ', '', 1)\n",
    "#thousandone_movies['title_edit'] = thousandone_movies['title_edit'].str.replace('Die ', '', 1)\n",
    "#thousandone_movies['title_edit'] = thousandone_movies['title_edit'].str.replace('Das ', '', 1)\n",
    "#thousandone_movies['title_edit'] = thousandone_movies['title_edit'].str.replace('La ', '', 1)\n",
    "#thousandone_movies['title_edit'] = thousandone_movies['title_edit'].str.replace('Un ', '', 1)\n",
    "#print(thousandone_movies.head(50))\n",
    "\n",
    "# Correcting for preposition order in Grouplens (where they are put at end)\n",
    "#ed_large_movies['title_edit'] = ed_large_movies['title'].str.replace('A ', '', 1)\n",
    "#ed_large_movies['title_edit'] = ed_large_movies['title_edit'].str.replace(', The ', ' ', 1)\n",
    "#ed_large_movies['title_edit'] = ed_large_movies['title_edit'].str.replace(', Les ', ' ', 1)\n",
    "#ed_large_movies['title_edit'] = ed_large_movies['title_edit'].str.replace(', Le ', ' ', 1)\n",
    "#ed_large_movies['title_edit'] = ed_large_movies['title_edit'].str.replace(', Die ', ' ', 1)\n",
    "#ed_large_movies['title_edit'] = ed_large_movies['title_edit'].str.replace('Das ', ' ', 1)\n",
    "#ed_large_movies['title_edit'] = ed_large_movies['title_edit'].str.replace(', La ', ' ', 1)\n",
    "#ed_large_movies['title_edit'] = ed_large_movies['title_edit'].str.replace(', Un ', ' ', 1)\n",
    "#print(ed_large_movies.head(50))\n",
    "\n",
    "# Then remove some other errors\n",
    "# Removing capitalization\n",
    "\n",
    "#thousandone_movies['title_edit'] = thousandone_movies['title_edit'].str.lower()\n",
    "#ed_large_movies['title_edit'] = ed_large_movies['title_edit'].str.lower()\n",
    "\n",
    "# Correcting for punctuation errors\n",
    "#thousandone_movies['title_edit'] = thousandone_movies['title_edit'].str.replace(',', '', 1)\n",
    "#ed_large_movies['title_edit'] = ed_large_movies['title_edit'].str.replace(',', '', 1)\n",
    "#thousandone_movies['title_edit'] = thousandone_movies['title_edit'].str.replace(':', '', 1)\n",
    "#ed_large_movies['title_edit'] = ed_large_movies['title_edit'].str.replace(':', '', 1)\n",
    "\n",
    "# Checking new match number\n",
    "#ed_large_join = thousandone_movies.merge(ed_large_movies, \n",
    "#                                        on='title_edit', how='left')\n",
    "# And now go through those files again to see what errors exist\n",
    "#ed_large_join.to_csv('Joined_To_Check.csv')\n",
    "#ed_large_movies.to_csv('GrouplensLargeTransformed.csv')\n",
    "# Demonstrating how many matched in small file and large file\n",
    "#ed_large_join.dropna().info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My basic string methods above led to an 88% match rate, but the nlkt package has a tokenizer that would be even better. The tokenizer led to a 97% match rate (only 25 movies were mismatched). On inspection, only x had a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', 'the', 'and']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jonathangerber/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Here's the tokenizer version.\n",
    "# import the right package\n",
    "import nltk\n",
    "# sometimes the line below needed to be included (not sure why)\n",
    "#from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "# Make default stopwords a list of punctuation\n",
    "stopwords = list(string.punctuation)\n",
    "# Add a few English words of no use (the default English list contains too many words)\n",
    "stopwords.append('the')\n",
    "stopwords.append('and')\n",
    "#print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make the tokenizer below and then apply it to all four datasets (three Grouplens and the 1,001 list). For ease I did the largest data set first, and the other two are in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a line of test code for my tokenizer\n",
    "#s = ['la voyage de la ( ) luns a trip to the moon','bread and milk . ,','la Voyage de la luns a trip to the moon']\n",
    "\n",
    "# Define tokenizer\n",
    "def my_tokenizer(title_list, stopwords):\n",
    "    '''This function takes a string and a list of stopwords and returns a list of word tokens\n",
    "    excluding anything defined in stopwords'''\n",
    "    # tokenize a lower-case string\n",
    "    s_token = [word_tokenize(i.lower()) for i in title_list]\n",
    "    # then filter out the stopwords\n",
    "    s_filtered=[]\n",
    "    for sentence in s_token:\n",
    "        s_filt = [w for w in sentence if not w in stopwords]\n",
    "        s_filtered.append(s_filt)\n",
    "    # finally sort them\n",
    "    s_sort = [sorted(item, key = lambda item:item[0]) for item in s_filtered] \n",
    "    return s_sort\n",
    "\n",
    "# Call my function on two datasets\n",
    "thousandone_movies['title_tokens'] = my_tokenizer(thousandone_movies['title'], stopwords)\n",
    "ed_large_movies['title_tokens'] = my_tokenizer(ed_large_movies['title'], stopwords)\n",
    "\n",
    "# To make matching easy I turn them into a string (I kept the old list version in case I wanted\n",
    "#to use a matching algorithm)\n",
    "thousandone_movies['title_string'] = thousandone_movies['title_tokens'].apply(', '.join)\n",
    "ed_large_movies['title_string'] = ed_large_movies['title_tokens'].apply(', '.join)\n",
    "# Then I join them\n",
    "ed_large_join = thousandone_movies.merge(ed_large_movies, \n",
    "                                        on='title_string', how='left')\n",
    "ed_large_join.to_csv('TOkenized_To_Check.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen below, the smaller datasets still had matches at 76% and 92%. This suggests the routine is working well (these are well up from the previous rates) and that these sets do not contain all of the 1,001 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 764 entries, 1 to 1221\n",
      "Data columns (total 7 columns):\n",
      "title_x           764 non-null object\n",
      "title_tokens_x    764 non-null object\n",
      "title_string      764 non-null object\n",
      "movieId           764 non-null float64\n",
      "title_y           764 non-null object\n",
      "genres            764 non-null object\n",
      "title_tokens_y    764 non-null object\n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 47.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 923 entries, 1 to 1177\n",
      "Data columns (total 7 columns):\n",
      "title_x           923 non-null object\n",
      "title_tokens_x    923 non-null object\n",
      "title_string      923 non-null object\n",
      "movieId           923 non-null float64\n",
      "title_y           923 non-null object\n",
      "genres            923 non-null object\n",
      "title_tokens_y    923 non-null object\n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 57.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# And do the other two as well\n",
    "new_movies['title_tokens'] = my_tokenizer(new_movies['title'], stopwords)\n",
    "new_movies['title_string'] = new_movies['title_tokens'].apply(', '.join)\n",
    "\n",
    "ed_small_movies['title_tokens'] = my_tokenizer(ed_small_movies['title'], stopwords)\n",
    "ed_small_movies['title_string'] = ed_small_movies['title_tokens'].apply(', '.join)\n",
    "\n",
    "ed_small_join = thousandone_movies.merge(ed_small_movies, \n",
    "                                        on='title_string', how='left')\n",
    "#ed_small_join.to_csv('TOkenized_To_Check.csv')\n",
    "new_movies_join = thousandone_movies.merge(new_movies, \n",
    "                                        on='title_string', how='left')\n",
    "#new_movies_join.to_csv('TOkenized_To_Check.csv')\n",
    "print(ed_small_join.dropna().info())\n",
    "print(new_movies_join.dropna().info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FROM HERE ON IS UNFINISHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some exploratory analyses of differences between 1,001 and non-1,001 movies (and The Dekalog and The Dark Knight), potentially by using demographic information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running a basic recommender model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate as tc #this library runs recommender systems\n",
    "\n",
    "training_data, validation_data = tc.recommender.util.random_split_by_user(actions, 'userId', 'movieId')\n",
    "baseline_model = tc.recommender.create(training_data, 'userId', 'movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_model = tc.recommender.create(training_data, 'userId', 'movieId', target='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparing the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer = tc.recommender.util.compare_models(validation_data, [baseline_model, ratings_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "References for turicreate:\n",
    "The basic user guide             https://apple.github.io/turicreate/docs/userguide/recommender/\n",
    "The recommender documentation    https://apple.github.io/turicreate/docs/api/turicreate.toolkits.recommender.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 974 entries, 1 to 1222\n",
      "Data columns (total 7 columns):\n",
      "title_x           974 non-null object\n",
      "title_tokens_x    974 non-null object\n",
      "title_string      974 non-null object\n",
      "movieId           974 non-null float64\n",
      "title_y           974 non-null object\n",
      "genres            974 non-null object\n",
      "title_tokens_y    974 non-null object\n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 60.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(ed_large_join.dropna().info())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
