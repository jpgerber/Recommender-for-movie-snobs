{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving movie recommendations for movie snobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project examines how to improve the precision of recommender systems for movie snobs.\n",
    "\n",
    "I will go through the following tasks:\n",
    "1. Collate the 1,001 Movies list (this will involve some wrangling)\n",
    "2. Select a dataset that has most of the 1,001 Movies in it\n",
    "3. Do some baseline modeling\n",
    "4. Run a baseline model and then a model with 1,001 as a topic/rating.\n",
    "5. Compare the models (with a 10% holdout set)\n",
    "6. Try various models until we can predict the difference between The Dark Knight and The Dekalog (good blog post title right there)\n",
    "\n",
    "Other ideas:\n",
    "1. Work out how long it takes for something to be canonical and then use this as a bias adjustment.\n",
    "2. use imdb api to find difference b/w dek and dk reviews (base it on the mini-project for naive bayes section)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five datasets of relevance on the Grouplens website.\n",
    "\n",
    "| Name | Movies | Ratings | Userdata | Release date |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| New research | 27K | 20M | None | 2016 |\n",
    "| Education (small) | 100K | 9K | None | 2018 |\n",
    "| Education (large) | 27M | 58K | None | 2018 |\n",
    "| Older (100K) | 1.7K | 100K| age, gender, occupation, zip | 1998 |\n",
    "| Older (1M) |  4K | 1M | age, gender, occupation, zip | 2003 |\n",
    "\n",
    "The latter two are useful for identifying people, the second and fourth are best if trying to look at changes in movie status across time.\n",
    "\n",
    "The relative percentages of the 1,001 Movies in each will be the first thing to work out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests, zipfile, io\n",
    "from bs4 import BeautifulSoup\n",
    "# Importing the ratings (no demogs) data\n",
    "# First download and extract the files (there's a bunch so use a list and loop)\n",
    "list_of_urls = ['http://files.grouplens.org/datasets/movielens/ml-20m.zip',\n",
    "               'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip',\n",
    "               'http://files.grouplens.org/datasets/movielens/ml-latest.zip',\n",
    "               'http://files.grouplens.org/datasets/movielens/ml-100k.zip',\n",
    "               'http://files.grouplens.org/datasets/movielens/ml-1m.zip']\n",
    "for url in list_of_urls:\n",
    "    ratings_small_file = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(ratings_small_file.content))\n",
    "    z.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the 1,001 list and converting it to what we want\n",
    "snob_url = 'https://1001films.fandom.com/wiki/The_List'\n",
    "snob_text= requests.get(snob_url)\n",
    "soup = BeautifulSoup(snob_text.content, 'html.parser')\n",
    "basic_list = (soup.body.find_all('b'))\n",
    "thousand_list = [item.text for item in basic_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title\n",
      "1  A Trip to the Moon (Le Voyage Dans La Lune) (1...\n",
      "2                     The Great Train Robbery (1903)\n",
      "3                       The Birth of a Nation (1915)\n",
      "4                                Les Vampires (1915)\n",
      "5                                 Intolerance (1916)\n",
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n",
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n"
     ]
    }
   ],
   "source": [
    "# Here I get all three files into dataframes\n",
    "# first, read the correct downloaded csvs (from separate subfolders)\n",
    "small_set_movies = pd.read_csv('ml-latest-small/movies.csv', sep = ',', header = 0)\n",
    "large_set_movies = pd.read_csv('ml-20m/movies.csv', sep = ',', header = 0)\n",
    "# Converting the 1,001 list to a dataframe and dropping the header row\n",
    "thousandone_movies = pd.DataFrame(thousand_list, columns = ['title']).drop(0)\n",
    "\n",
    "# The following code can be modified to check they are all dataframes\n",
    "#print(type(small_set_movies))\n",
    "# Looking at head of all three\n",
    "print(thousandone_movies.head())\n",
    "print(small_set_movies.head())\n",
    "print(large_set_movies.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No doubt the hardest part will be adjusting for any naming discrepancies between files. So I guess we just join the largest sets to the smallest to check how many match. And then keep all the non-matched as well. Any joins that have a resulting movieId will be correctly matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FROM HERE ON IS UNFINISHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM\n",
    "# there are three main datasets, one for research with 20m, and a small one (9K) and large for development (58K)        \n",
    "genome_score                               \n",
    "genome_tags\n",
    "links\n",
    "movies\n",
    "ratings\n",
    "tags\n",
    "# Working out how many of the 1,001 are in the Grouplens sets\n",
    "#20m has 27,000 movies\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a basic model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate as tc #this model runs recommender systems\n",
    "\n",
    "training_data, validation_data = tc.recommender.util.random_split_by_user(actions, 'userId', 'movieId')\n",
    "baseline_model = tc.recommender.create(training_data, 'userId', 'movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_model = tc.recommender.create(training_data, 'userId', 'movieId', target='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer = tc.recommender.util.compare_models(validation_data, [baseline_model, ratings_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "References:\n",
    "The basic user guide             https://apple.github.io/turicreate/docs/userguide/recommender/\n",
    "The recommender documentation    https://apple.github.io/turicreate/docs/api/turicreate.toolkits.recommender.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
